{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPiRXxYg5H1ClQmgmSB8Ivd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NOTGOD6000/Web-crawler-Email-Scraper-/blob/main/web%20crawler%20email%20scraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from urllib.parse import urljoin, urlparse\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import threading\n",
        "import csv\n",
        "\n",
        "# Starting point for the crawler\n",
        "start_url = \"https://www.punjabiuniversity.ac.in/pages/Courses.aspx?Id=18\"\n",
        "\n",
        "# Configuration: Maximum pages and depth to crawl\n",
        "max_pages = 100\n",
        "max_depth = 3\n",
        "\n",
        "# Thread-safe sets and a lock for shared data\n",
        "lock = threading.Lock()\n",
        "visited = set([start_url])  # URLs already visited\n",
        "emails_found = set()        # Emails found across pages\n",
        "\n",
        "def extract_emails_and_links(url, base_url):\n",
        "    \"\"\"\n",
        "    Downloads the webpage, extracts emails and internal links.\n",
        "    Returns a tuple of (emails, links).\n",
        "    \"\"\"\n",
        "    emails_local = set()\n",
        "    links_local = set()\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "        # Extract emails using regex\n",
        "        text = soup.get_text()\n",
        "        emails_local.update(re.findall(r\"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+\", text))\n",
        "\n",
        "        # Extract and normalize internal links\n",
        "        for link in soup.find_all(\"a\", href=True):\n",
        "            href = link['href']\n",
        "            full_url = urljoin(base_url, href)\n",
        "            parsed = urlparse(full_url)\n",
        "\n",
        "            # Only follow internal links (same domain)\n",
        "            if parsed.netloc == urlparse(base_url).netloc:\n",
        "                links_local.add(full_url)\n",
        "    except Exception:\n",
        "        pass  # Ignore any errors (timeouts, connection issues, etc.)\n",
        "    return emails_local, links_local\n",
        "\n",
        "def crawl(url, base_url, depth):\n",
        "    \"\"\"\n",
        "    Crawls a given URL to extract emails and discover new links to follow.\n",
        "    Returns a list of new links to crawl next.\n",
        "    \"\"\"\n",
        "    global visited, emails_found\n",
        "    if depth > max_depth:\n",
        "        return []\n",
        "\n",
        "    # Extract emails and links from the current page\n",
        "    emails_local, links_local = extract_emails_and_links(url, base_url)\n",
        "\n",
        "    # Add found emails to the global set\n",
        "    with lock:\n",
        "        emails_found.update(emails_local)\n",
        "\n",
        "    new_links = []\n",
        "    with lock:\n",
        "        for link in links_local:\n",
        "            # Add new unvisited internal links, if limit not exceeded\n",
        "            if link not in visited and len(visited) < max_pages:\n",
        "                visited.add(link)\n",
        "                new_links.append((link, base_url, depth + 1))\n",
        "    return new_links\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Orchestrates the crawling using a thread pool for concurrency.\n",
        "    \"\"\"\n",
        "    to_crawl = [(start_url, start_url, 0)]  # Queue of (URL, base URL, depth)\n",
        "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
        "        futures = []\n",
        "        while to_crawl:\n",
        "            # Submit crawl tasks\n",
        "            for args in to_crawl:\n",
        "                futures.append(executor.submit(crawl, *args))\n",
        "            to_crawl = []\n",
        "\n",
        "            # Wait for all current tasks to complete and collect new links\n",
        "            for future in as_completed(futures):\n",
        "                new_links = future.result()\n",
        "                to_crawl.extend(new_links)\n",
        "\n",
        "            # Clear futures for the next batch\n",
        "            futures = []\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "    # Save the found emails to a CSV file\n",
        "    if emails_found:\n",
        "        print(f\"✅ Found {len(emails_found)} emails. Saving to emails.csv ...\")\n",
        "        with open(\"emails.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow([\"Email\"])\n",
        "            for email in sorted(emails_found):\n",
        "                writer.writerow([email])\n",
        "        print(\"✅ Saved emails to emails.csv\")\n",
        "    else:\n",
        "        print(\"⚠️ No emails found.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzNnfQwsU_tz",
        "outputId": "f6cb0997-c0f4-412c-e6f3-82321ea263f2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Found 207 emails. Saving to emails.csv ...\n",
            "✅ Saved emails to emails.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from urllib.parse import urljoin, urlparse\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import threading\n",
        "\n",
        "# Starting URL to begin crawling\n",
        "start_url = \"https://www.punjabiuniversity.ac.in/\"\n",
        "\n",
        "# Limits to control crawl size\n",
        "max_pages = 30  # Maximum number of pages to visit\n",
        "max_depth = 2   # Maximum depth of link traversal from the start URL\n",
        "\n",
        "# Thread lock to prevent race conditions when accessing shared data\n",
        "lock = threading.Lock()\n",
        "\n",
        "# Sets to keep track of visited URLs and found email addresses\n",
        "visited = set([start_url])\n",
        "emails_found = set()\n",
        "\n",
        "# Not currently used but declared - might be useful for future extension\n",
        "tasks = []\n",
        "\n",
        "def extract_emails_and_links(url, base_url, depth):\n",
        "    \"\"\"\n",
        "    Fetches a web page and extracts all emails and internal links.\n",
        "    Returns:\n",
        "        - emails_local: A set of email addresses found\n",
        "        - links_local: A set of internal URLs found\n",
        "    \"\"\"\n",
        "    emails_local = set()\n",
        "    links_local = set()\n",
        "    try:\n",
        "        # Send HTTP GET request to fetch the page\n",
        "        response = requests.get(url, timeout=10)\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "        # Extract text and search for emails using regex\n",
        "        text = soup.get_text()\n",
        "        emails_local.update(re.findall(\n",
        "            r\"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+\", text))\n",
        "\n",
        "        # Extract all <a href=\"\"> links\n",
        "        for link in soup.find_all(\"a\", href=True):\n",
        "            href = link['href']\n",
        "            full_url = urljoin(base_url, href)  # Convert to absolute URL\n",
        "            parsed = urlparse(full_url)\n",
        "\n",
        "            # Only consider internal links (same domain)\n",
        "            if parsed.netloc == urlparse(base_url).netloc:\n",
        "                links_local.add(full_url)\n",
        "    except Exception:\n",
        "        # Skip pages that fail to load (e.g., timeout, 404)\n",
        "        pass\n",
        "    return emails_local, links_local\n",
        "\n",
        "def crawl(url, base_url, depth):\n",
        "    \"\"\"\n",
        "    Crawls the specified URL:\n",
        "    - Extracts emails and new internal links\n",
        "    - Returns a list of new links to crawl (if within limits)\n",
        "    \"\"\"\n",
        "    global visited, emails_found\n",
        "\n",
        "    if depth > max_depth:\n",
        "        return []  # Don't go beyond the allowed depth\n",
        "\n",
        "    # Extract emails and links from the page\n",
        "    emails_local, links_local = extract_emails_and_links(url, base_url, depth)\n",
        "\n",
        "    # Safely update the global emails set\n",
        "    with lock:\n",
        "        emails_found.update(emails_local)\n",
        "\n",
        "    new_links = []\n",
        "    with lock:\n",
        "        # Check for new links that haven't been visited and within page limit\n",
        "        for link in links_local:\n",
        "            if link not in visited and len(visited) < max_pages:\n",
        "                visited.add(link)\n",
        "                new_links.append((link, base_url, depth + 1))\n",
        "    return new_links\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main controller function that manages the crawling process\n",
        "    using a thread pool for concurrent crawling.\n",
        "    \"\"\"\n",
        "    to_crawl = [(start_url, start_url, 0)]  # Initialize with start URL\n",
        "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
        "        futures = []\n",
        "\n",
        "        while to_crawl:\n",
        "            # Submit crawl tasks to the executor\n",
        "            for args in to_crawl:\n",
        "                futures.append(executor.submit(crawl, *args))\n",
        "\n",
        "            to_crawl = []  # Clear the queue for next round\n",
        "\n",
        "            # Process completed crawl tasks\n",
        "            for future in as_completed(futures):\n",
        "                new_links = future.result()\n",
        "                to_crawl.extend(new_links)  # Queue up new links for crawling\n",
        "\n",
        "            futures = []  # Clear the futures list for next batch\n",
        "\n",
        "# Entry point of the script\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "    # Display results\n",
        "    if emails_found:\n",
        "        print(\"✅ Emails found:\")\n",
        "        for email in sorted(emails_found):\n",
        "            print(email)\n",
        "    else:\n",
        "        print(\"⚠️ No emails found.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EY7OtumwUJqE",
        "outputId": "769ef5fa-dd3b-42b2-9012-90fdd9d40e20"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Emails found:\n",
            "adminwebsite@pbi.ac.in\n",
            "admissions@pbi.ac.in\n",
            "bhiminderpbi@gmail.com\n",
            "brar_jas@yahoo.co.in\n",
            "campus.dehla@gmail.com\n",
            "campus.ralla@gmail.com\n",
            "camspup7@gmail.com\n",
            "ccsr.pup@gmail.com\n",
            "cdeispbi@yahoo.com\n",
            "ce2013pup@yahoo.com\n",
            "cepwd.pup@gmail.com\n",
            "coemrampuraphul@gmail.com\n",
            "coordinator_nss@yahoo.com\n",
            "daapup@pbi.ac.in\n",
            "daljit_ahluwalia@yahoo.com\n",
            "dbs@pbi.ac.in\n",
            "dbsskdn@gmail.com\n",
            "dcs@pbi.ac.in\n",
            "dean.alumnirelations@gmail.com\n",
            "deanalumni@pbi.ac.in\n",
            "deanandheadlaw@gmail.com\n",
            "deanartsandculture.pup@gmail.com\n",
            "deanstudentswelfare6@gmail.com\n",
            "deepskandhala@yahoo.co.in\n",
            "departmenthistory2015@gmail.com\n",
            "diapupadmission@pbi.ac.in\n",
            "director@pbi.ac.in\n",
            "director@wscpedia.org\n",
            "director_dias@pbi.ac.in\n",
            "directorthhmpup@gmail.com\n",
            "directoryouthwelfaredept@gmail.com\n",
            "dirsports@pbi.ac.in\n",
            "dispup2016@gmail.com\n",
            "dispup2016@pbi.ac.in\n",
            "dpm@pbi.ac.in\n",
            "edcellpup@gmail.com\n",
            "edrcbti@gmail.com\n",
            "eich@pbi.ac.in\n",
            "emmrc.patiala@gmail.com\n",
            "etranscript@pbi.ac.in\n",
            "foreignlanguages@pbi.ac.in\n",
            "gurmatsangeetchair@pbi.ac.in\n",
            "gurmatsangeetonline@pbi.ac.in\n",
            "head_biotechnology@pbi.ac.in\n",
            "head_botany@pbi.ac.in\n",
            "head_bvsc@pbi.ac.in\n",
            "head_dl@pbi.ac.in\n",
            "head_economics@pbi.ac.in\n",
            "head_finearts@pbi.ac.in\n",
            "head_forensicscience@pbi.ac.in\n",
            "head_geography@pbi.ac.in\n",
            "head_gurmatsangeet@pbi.ac.in\n",
            "head_linglex@pbi.ac.in\n",
            "head_lis@pbi.ac.in\n",
            "head_mba@pbi.ac.in\n",
            "head_med@pbi.ac.in\n",
            "head_pharmacy@pbi.ac.in\n",
            "head_physics@pbi.ac.in\n",
            "head_polsci@pbi.ac.in\n",
            "head_psychology@pbi.ac.in\n",
            "head_publicadmin@pbi.ac.in\n",
            "head_punjabi_ls@pbi.ac.in\n",
            "head_sam@pbi.ac.in\n",
            "head_sportssc@pbi.ac.in\n",
            "head_ycoe@pbi.ac.in\n",
            "head_zoology@pbi.ac.in\n",
            "headbhaigurdaschair@gmail.com\n",
            "headchemistrypup@gmail.com\n",
            "headcommerce@pbi.ac.in\n",
            "headddepbi@yahoo.com\n",
            "headeducationpup@gmail.com\n",
            "headjaito@pbi.ac.in\n",
            "headofdpd@gmail.com\n",
            "headphilosophy@pbi.ac.in\n",
            "headphysiotherapypup@pbi.ac.in\n",
            "headpunjabi@gmail.com\n",
            "headsggss@gmail.com\n",
            "headttv@pbi.ac.in\n",
            "hgenpup@gmail.com\n",
            "hindidepartmentpup@gmail.com\n",
            "hjejishergill@gmail.com\n",
            "hod_dance@pbi.ac.in\n",
            "hodmathspup@gmail.com\n",
            "hrdcpup@gmail.com\n",
            "iastrainingcentrepbi@gmail.com\n",
            "info@gurmatsangeetlibrary.com\n",
            "iqacell@gmail.com\n",
            "kanwalgrewal11@gmail.com\n",
            "libksnl@pbi.ac.in\n",
            "maharajaagrasenchairpup@gmail.com\n",
            "maharanapratapchairpta@gmail.com\n",
            "msgcmansa@yahoo.com\n",
            "nancy_jmc@pbi.ac.in\n",
            "neelgagan2k3@pbi.ac.in\n",
            "nsmkinst@yahoo.com\n",
            "office_civil@pbi.ac.in\n",
            "pbiunibddncj@pbi.ac.in\n",
            "pchc@pbi.ac.in\n",
            "phyedu@pbi.ac.in\n",
            "placementcellpup@gmail.com\n",
            "pucbahadarpur77@gmail.com\n",
            "pucdhilwan@gmail.com\n",
            "puceit@pbi.ac.in\n",
            "pucmaur@hotmail.com\n",
            "pucmoonak@pbi.ac.in\n",
            "pup.collegesection@gmail.com\n",
            "pup.english@gmail.com\n",
            "pup_estt@pbi.ac.in\n",
            "purcitm@pbi.ac.in\n",
            "rajwinderjhunir@yahoo.com\n",
            "rc_law@pbi.ac.in\n",
            "rcbathinda@gmail.com\n",
            "registrar@pbi.ac.in\n",
            "researchpup@gmail.com\n",
            "rstudiespatiala@gmail.com\n",
            "rticell@pbi.ac.in\n",
            "sandhuph@yahoo.com\n",
            "sanjivpuriucoe@gmail.com\n",
            "sanskrit6472@gmail.com\n",
            "sicpupatiala@gmail.com\n",
            "socialworkpup@gmail.com\n",
            "sosspup@gmail.com\n",
            "suficentrepup@gmail.com\n",
            "tpdmalwa@gmail.com\n",
            "transportpup@gmail.com\n",
            "uc.benra2016@gmail.com\n",
            "ucck.pbi2011@yahoo.com\n",
            "ucg84@ymail.com\n",
            "ucghanaur@pbi.ac.in\n",
            "ucjaito2014@gmail.com\n",
            "universitycollegemiranpur@gmail.com\n",
            "urdudeptpup@gmail.com\n",
            "usbstalwandisabo@gmail.com\n",
            "usicpup@gmail.com\n",
            "valmikichairpup@gmail.com\n",
            "vc@pbi.ac.in\n",
            "vishal.pup@gmail.com\n",
            "vishal_cs@pbi.ac.in\n",
            "vpic@pbi.ac.in\n",
            "ydoshead@pbi.ac.in\n"
          ]
        }
      ]
    }
  ]
}